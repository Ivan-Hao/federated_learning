{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/tony92151/c952ae59715d439533ced2c4621d28e7/federated-learning-with-pysyft-and-pytorch-colab-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTzXxO8lU9BH",
        "colab_type": "text"
      },
      "source": [
        "## Useful imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:40.160008Z",
          "start_time": "2019-06-03T19:33:39.344527Z"
        },
        "id": "A1maeg9VU9BI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:40.172746Z",
          "start_time": "2019-06-03T19:33:40.163793Z"
        },
        "id": "aAE2RujLU9BM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inputs = np.load('data/inputs.npy')\n",
        "labels = np.load('data/labels.npy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEsXwDXrsvdT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "0371365d-9bcb-488e-e38a-d2afbc5b8160"
      },
      "source": [
        "print(inputs[:2])\n",
        "print(labels[:2])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   0    0    0    0    0    0    0    0    0    0 4202 6580  241 4826\n",
            "  7525 1431 3677 5707 8491 3171 7978 4060 6249 3616 4345  771 4445 7931\n",
            "  1315 5303]\n",
            " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "     0    0    0    0    0    0    0    0    0    0 4332 3155 7400  210\n",
            "  4718 8098]]\n",
            "[0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:40.182467Z",
          "start_time": "2019-06-03T19:33:40.176346Z"
        },
        "id": "JCT5rMIRU9BO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = int(inputs.max()) + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:40.207763Z",
          "start_time": "2019-06-03T19:33:40.201292Z"
        },
        "id": "Ii7057cHU9BR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training params\n",
        "EPOCHS = 15\n",
        "CLIP = 5 # gradient clipping - to avoid gradient explosion (frequent in RNNs)\n",
        "lr = 0.1\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "# Model params\n",
        "EMBEDDING_DIM = 50\n",
        "HIDDEN_DIM = 10\n",
        "DROPOUT = 0.2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:41.963634Z",
          "start_time": "2019-06-03T19:33:40.212236Z"
        },
        "id": "EZ6rAMuMU9BU",
        "colab_type": "code",
        "outputId": "9bda0dbd-3223-4018-8d58-4c1d2bd3774e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import syft as sy"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:40.197935Z",
          "start_time": "2019-06-03T19:33:40.186270Z"
        },
        "id": "R5j6qmBnU9BV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = torch.tensor(labels)\n",
        "inputs = torch.tensor(inputs)\n",
        "\n",
        "# splitting training and test data\n",
        "pct_test = 0.2\n",
        "\n",
        "train_labels = labels[:-int(len(labels)*pct_test)]\n",
        "train_inputs = inputs[:-int(len(labels)*pct_test)]\n",
        "\n",
        "test_labels = labels[-int(len(labels)*pct_test):]\n",
        "test_inputs = inputs[-int(len(labels)*pct_test):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:42.591430Z",
          "start_time": "2019-06-03T19:33:41.969220Z"
        },
        "id": "Y7uWmDhhU9BX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hook that extends the Pytorch library to enable all computations with pointers of tensors sent to other workers\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Creating 2 virtual workers\n",
        "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
        "anne = sy.VirtualWorker(hook, id=\"anne\")\n",
        "\n",
        "# threshold indexes for dataset split (one half for Bob, other half for Anne)\n",
        "train_idx = int(len(train_labels)/2)\n",
        "test_idx = int(len(test_labels)/2)\n",
        "\n",
        "# Sending toy datasets to virtual workers\n",
        "bob_train_dataset = sy.BaseDataset(train_inputs[:train_idx], train_labels[:train_idx]).send(bob)\n",
        "anne_train_dataset = sy.BaseDataset(train_inputs[train_idx:], train_labels[train_idx:]).send(anne)\n",
        "bob_test_dataset = sy.BaseDataset(test_inputs[:test_idx], test_labels[:test_idx]).send(bob)\n",
        "anne_test_dataset = sy.BaseDataset(test_inputs[test_idx:], test_labels[test_idx:]).send(anne)\n",
        "\n",
        "# Creating federated datasets, an extension of Pytorch TensorDataset class\n",
        "federated_train_dataset = sy.FederatedDataset([bob_train_dataset, anne_train_dataset])\n",
        "federated_test_dataset = sy.FederatedDataset([bob_test_dataset, anne_test_dataset])\n",
        "\n",
        "# Creating federated dataloaders, an extension of Pytorch DataLoader class\n",
        "federated_train_loader = sy.FederatedDataLoader(federated_train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "federated_test_loader = sy.FederatedDataLoader(federated_test_dataset, shuffle=False, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ole91lfuU9BZ",
        "colab_type": "text"
      },
      "source": [
        "### Creating simple GRU (1-layer) model with sigmoid activation for classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOmSAKw6U9BZ",
        "colab_type": "text"
      },
      "source": [
        "Unfortunatelly, the current version of PySyft does not support the RNNs modules of PyTorch yet. However, I was able to handcraft a simple GRU network with linear layers for this project. \n",
        "\n",
        "As the focus of this notebook is the usage of PySyft, I will skip the construction of the model. If you are interested in how I built the model, you can check it out on [handcrafted_GRU.py](https://github.com/andrelmfarias/Private-AI/blob/master/Federated_Learning/handcrafted_GRU.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:42.613017Z",
          "start_time": "2019-06-03T19:33:42.598004Z"
        },
        "id": "jiHlt_AyU9Ba",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from handcrafted_GRU import GRU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:33:42.638046Z",
          "start_time": "2019-06-03T19:33:42.617601Z"
        },
        "id": "NI3reLoTU9Bb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initiating the model\n",
        "model = GRU(vocab_size=VOCAB_SIZE, hidden_dim=HIDDEN_DIM, embedding_dim=EMBEDDING_DIM, dropout=DROPOUT)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVFk9Mq2U9Bd",
        "colab_type": "text"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J06t7n9U9Be",
        "colab_type": "text"
      },
      "source": [
        "For now, PySyft does not support optimizers with momentum. Therefore, we are going to stick with the classical [Stochastic Gradient Descent](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD) optimizer.\n",
        "\n",
        "As our task consists of a binary classification, we are going to use the [Binary Cross Entropy Loss](https://pytorch.org/docs/stable/nn.html#torch.nn.BCELoss)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T20:00:23.084933Z",
          "start_time": "2019-06-03T20:00:23.078688Z"
        },
        "id": "x1O-wTd5U9Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmPqPbUlU9Bg",
        "colab_type": "text"
      },
      "source": [
        "For each epoch we are going to compute the training and validations losses, as well as the [Area Under the ROC Curve](https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics) score due to the fact that the target dataset is unbalaced (only 13% of labels are positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-06-03T19:56:01.459697Z",
          "start_time": "2019-06-03T19:33:42.666174Z"
        },
        "code_folding": [],
        "id": "7xvhmpNCU9Bg",
        "colab_type": "code",
        "outputId": "1a80edb9-e268-44bc-8023-c0a95a2ba261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for e in range(EPOCHS):\n",
        "    \n",
        "    ######### Training ##########\n",
        "    \n",
        "    losses = []\n",
        "    # Batch loop\n",
        "    for inputs, labels in federated_train_loader:\n",
        "        # Location of current batch\n",
        "        worker = inputs.location\n",
        "        # Initialize hidden state and send it to worker\n",
        "        h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)\n",
        "        # Send model to current worker\n",
        "        model.send(worker)\n",
        "        # Setting accumulated gradients to zero before backward step\n",
        "        optimizer.zero_grad()\n",
        "        # Output from the model\n",
        "        output, _ = model(inputs, h)\n",
        "        # Calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        # Clipping the gradient to avoid explosion\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "        # Backpropagation step\n",
        "        optimizer.step() \n",
        "        # Get the model back to the local worker\n",
        "        model.get()\n",
        "        losses.append(loss.get())\n",
        "    \n",
        "    ######## Evaluation ##########\n",
        "    \n",
        "    # Model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_preds = []\n",
        "        test_labels_list = []\n",
        "        eval_losses = []\n",
        "\n",
        "        for inputs, labels in federated_test_loader:\n",
        "            # get current location\n",
        "            worker = inputs.location\n",
        "            # Initialize hidden state and send it to worker\n",
        "            h = torch.Tensor(np.zeros((BATCH_SIZE, HIDDEN_DIM))).send(worker)    \n",
        "            # Send model to worker\n",
        "            model.send(worker)\n",
        "            \n",
        "            output, _ = model(inputs, h)\n",
        "            loss = criterion(output.squeeze(), labels.float())\n",
        "            eval_losses.append(loss.get())\n",
        "            preds = output.squeeze().get()\n",
        "            test_preds += list(preds.numpy())\n",
        "            test_labels_list += list(labels.get().numpy().astype(int))\n",
        "            # Get the model back to the local worker\n",
        "            model.get()\n",
        "        \n",
        "        score = roc_auc_score(test_labels_list, test_preds)\n",
        "    \n",
        "    print(\"Epoch {}/{}...  \\\n",
        "    AUC: {:.3%}...  \\\n",
        "    Training loss: {:.5f}...  \\\n",
        "    Validation loss: {:.5f}\".format(e+1, EPOCHS, score, sum(losses)/len(losses), sum(eval_losses)/len(eval_losses)))\n",
        "    \n",
        "    model.train()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15...      AUC: 72.618%...      Training loss: 0.40979...      Validation loss: 0.35500\n",
            "Epoch 2/15...      AUC: 79.190%...      Training loss: 0.35325...      Validation loss: 0.32535\n",
            "Epoch 3/15...      AUC: 84.579%...      Training loss: 0.31621...      Validation loss: 0.28853\n",
            "Epoch 4/15...      AUC: 89.720%...      Training loss: 0.28319...      Validation loss: 0.24455\n",
            "Epoch 5/15...      AUC: 93.667%...      Training loss: 0.22717...      Validation loss: 0.19267\n",
            "Epoch 6/15...      AUC: 95.531%...      Training loss: 0.17956...      Validation loss: 0.15372\n",
            "Epoch 7/15...      AUC: 96.034%...      Training loss: 0.14125...      Validation loss: 0.13412\n",
            "Epoch 8/15...      AUC: 96.384%...      Training loss: 0.11721...      Validation loss: 0.12582\n",
            "Epoch 9/15...      AUC: 96.709%...      Training loss: 0.10556...      Validation loss: 0.11969\n",
            "Epoch 10/15...      AUC: 96.954%...      Training loss: 0.08936...      Validation loss: 0.11111\n",
            "Epoch 11/15...      AUC: 97.019%...      Training loss: 0.07768...      Validation loss: 0.11159\n",
            "Epoch 12/15...      AUC: 96.605%...      Training loss: 0.07326...      Validation loss: 0.14263\n",
            "Epoch 13/15...      AUC: 96.989%...      Training loss: 0.06321...      Validation loss: 0.11013\n",
            "Epoch 14/15...      AUC: 97.084%...      Training loss: 0.06039...      Validation loss: 0.10946\n",
            "Epoch 15/15...      AUC: 97.117%...      Training loss: 0.05421...      Validation loss: 0.11133\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb_PIyY4U9Bj",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWL_YdL1U9Bk",
        "colab_type": "text"
      },
      "source": [
        "We can see that with the PySyft library and its PyTorch extension, we can perform operations with tensor pointers such as we can do with PyTorch API (but for some limitations that are still to be addressed). \n",
        "\n",
        "Thanks to this, we were able to train spam detector model without having any access to the remote and private data: for each batch we sent the model to the current remote worker and got it back to the local machine before sending it to the worker of the next batch.\n",
        "\n",
        "We can also notice that this federated training did not harm the performance of the model as both losses reduced at each epoch as expected and the final AUC score on the test data was above 97.5%.\n",
        "\n",
        "There is however one limitation of this method: by getting the model back we can still have access to some private information. \n",
        "Let's say Bob had only one SMS on his machine. When we get the model back, we can just check which embeddings of the model changed and we will know which were the tokens (words) of the SMS.\n",
        "\n",
        "In order to address this issue, there are two solutions: Differential Privacy and Secured Multi-Party Computation (SMPC). Differential Privacy would be used to make sure the model does not give access to some private information. SMPC, which is one kind of Encrypted Computation, in return allows you to send the model privately so that the remote workers which have the data cannot see the weights you are using.\n",
        "\n",
        "I will show how can we perform these techniques with PySyft in a next tutorial.\n",
        "\n",
        "Et voil√†!"
      ]
    }
  ],
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Federated learning with Pysyft and Pytorch colab demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}